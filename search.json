[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modern Data Analysis",
    "section": "",
    "text": "By Sangyu Xu, Joses Ho, Yishan Mai, and Adam Claridge-Chang\n\nIntroducing biomedical scientists to data analysis with Python\nThe goal of this class is to introduce biomedical scientists to data analysis with Python notebooks. Ideally, this should be a semester-long course. This 2-hour class is presented to two groups.\n\nDuke-NUS PhD programme in Clinical and Translational Sciences. Foundations of Precision Medicine Hands-on Workshops (GMS6812).\nDuke-NUS PhD programme in Integrated Biology and Medicine. Ethics and Personal and Professional Development Sessions.\n\n\n\nThe class has the following steps\n\nPreparation 1. Read the Introduction and install the prerequisite software on your laptop.\nPreparation 2. Go through a Quick Tour of the Notebook to familiarise yoruself to the jupyter notebook environment.\nSession Coding. Bring your laptop to class, and we will go through the examples in Data Analysis with Jupyter.\nLecture. Crash course on the history of and key issues in data visualization.\nShort introduction to estimation and LLMs. Introduction to estimation statistics web app and python package.\nContinuing education. Review the Additional Resources page to continue your self-education in data analysis.\n\nIf you have any questions about these materials, please contact your course coordinator.",
    "crumbs": [
      "Modern Data Analysis"
    ]
  },
  {
    "objectID": "dabest_introduction.html",
    "href": "dabest_introduction.html",
    "title": "04. DABEST Introduction",
    "section": "",
    "text": "DABEST is a package that performs estimation statistics available on Python and R. With Jupyter Notebook you can try DABEST-Python.\n\nimport pandas as pd\nimport dabest\nfrom palmerpenguins import load_penguins\n\nPre-compiling numba functions for DABEST...\n\n\nCompiling numba functions: 100%|████████████████| 11/11 [00:00&lt;00:00, 67.69it/s]\n\n\nNumba compilation complete!\n\n\n\n\n\n\npenguins = load_penguins() \n\n# If you had trouble installing the penguins package, you can also read in the data from the csv file by uncommenting the line below.\n# penguins = pd.read_csv(\"penguins.csv\")\n\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\npenguins_analyse = dabest.load(data=penguins, \n                           x=\"species\", y=\"bill_length_mm\",\n                           idx=(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n                          )\n\n\npenguins_analyse.mean_diff\n\nDABEST v2025.03.27\n==================\n                  \nGood evening!\nThe current time is Thu Mar 27 23:02:20 2025.\n\nThe unpaired mean difference between Adelie and Chinstrap is 10.0 [95%CI 9.14, 11.0].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\nThe unpaired mean difference between Adelie and Gentoo is 8.71 [95%CI 8.02, 9.42].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis of zero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\n\npenguins_analyse.mean_diff.plot(raw_marker_size = 1.5, fig_size=(7, 7), raw_label=\"Bill length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# penguins.dropna().melt(id_vars=\"species\", value_vars=\"metric\").head()\n\n\nEven easier estimation statistics\nThe DABEST library has also been developed into a web application at estimationstats.com",
    "crumbs": [
      "04. DABEST Introduction"
    ]
  },
  {
    "objectID": "additional_resources.html",
    "href": "additional_resources.html",
    "title": "05. Additional resources",
    "section": "",
    "text": "Coding: There are many online resources to learn coding. Published in 2021, A Data-Centric Introduction to Computing uses a Python-like teaching language (Pyret) to introduce key concepts in computer science.\nThe command-line interface: You will want to learn the command line interface, we recommend The basics of working on the UNIX command line to start. Continue with the Bash shell tutorial.\nA text on data visualization: Claus Wilke’s free online book is a great introduction to data visualization, and a style guide. It is written in R, which is the best language for statistics.\nEstimation: Our estimationstats.com site has introductory information on estimation and specific types of analyses and effect sizes.\n\n\n\nTry using the estimationstats.com web app to analyze your own grouped data.\nOpen and have a look at the sample multivariate data. Go through the introductory notebook that demonstrates data analysis.\nWe recommend the following texts to strengthen your data-analysis and presentation skills. They can be dipped into over the coming months or years, and used as references. Being familiar with some or all of this material will help you write your first-author paper/s and doctoral thesis.\n\n\n\nEstimation: If you want to learn about estimation statistics in greater depth, there is Calin-Jageman and Cumming’s textbook that is well-written, funny, and clear. The authors also run a blog.\nEstimation: Christoph Bernard’s account of the pioneering experience of a major journal (eNeuro) recommending estimation as standard: the initial announcement, author feedback, and after one year.\nCoding: The paid coding tutorial Learn Python The Hard Way has a good reputation, but there are also many free options (see DCIC above) with great reviews.\nCoding: It will help to learn to use your computer’s Unix-style command-line shell. This interface will allow you to use package managers like conda and homebrew, version-control tools like git, and other important tools. There are many books about the shell, with only minor differences between MacOS, Windows, and Linux.\nDatavis: A brief guide to oral–visual data presentations (talks).\nDatavis: A reader-funded textbook on typography, including for slides. Since so much communication relies on text, typography is an important part of the data interface.\nDatavis: For historical perspectives, Edward Tufte’s books are classic texts to develop your design skills, and there is Friendly and Wainer’s History of Data Visualization.\nAs you progress, you will want to develop your skills in areas like bioinformatics, image processing, and/or machine learning. The iris dataset is widely used for training in multivariate data analysis, with many online tutorials.\nThe social reasons to learn programming also apply to programming for research.",
    "crumbs": [
      "05. Additional resources"
    ]
  },
  {
    "objectID": "additional_resources.html#key-resources-a-self-guided-course-in-data-science",
    "href": "additional_resources.html#key-resources-a-self-guided-course-in-data-science",
    "title": "05. Additional resources",
    "section": "",
    "text": "Coding: There are many online resources to learn coding. Published in 2021, A Data-Centric Introduction to Computing uses a Python-like teaching language (Pyret) to introduce key concepts in computer science.\nThe command-line interface: You will want to learn the command line interface, we recommend The basics of working on the UNIX command line to start. Continue with the Bash shell tutorial.\nA text on data visualization: Claus Wilke’s free online book is a great introduction to data visualization, and a style guide. It is written in R, which is the best language for statistics.\nEstimation: Our estimationstats.com site has introductory information on estimation and specific types of analyses and effect sizes.\n\n\n\nTry using the estimationstats.com web app to analyze your own grouped data.\nOpen and have a look at the sample multivariate data. Go through the introductory notebook that demonstrates data analysis.\nWe recommend the following texts to strengthen your data-analysis and presentation skills. They can be dipped into over the coming months or years, and used as references. Being familiar with some or all of this material will help you write your first-author paper/s and doctoral thesis.\n\n\n\nEstimation: If you want to learn about estimation statistics in greater depth, there is Calin-Jageman and Cumming’s textbook that is well-written, funny, and clear. The authors also run a blog.\nEstimation: Christoph Bernard’s account of the pioneering experience of a major journal (eNeuro) recommending estimation as standard: the initial announcement, author feedback, and after one year.\nCoding: The paid coding tutorial Learn Python The Hard Way has a good reputation, but there are also many free options (see DCIC above) with great reviews.\nCoding: It will help to learn to use your computer’s Unix-style command-line shell. This interface will allow you to use package managers like conda and homebrew, version-control tools like git, and other important tools. There are many books about the shell, with only minor differences between MacOS, Windows, and Linux.\nDatavis: A brief guide to oral–visual data presentations (talks).\nDatavis: A reader-funded textbook on typography, including for slides. Since so much communication relies on text, typography is an important part of the data interface.\nDatavis: For historical perspectives, Edward Tufte’s books are classic texts to develop your design skills, and there is Friendly and Wainer’s History of Data Visualization.\nAs you progress, you will want to develop your skills in areas like bioinformatics, image processing, and/or machine learning. The iris dataset is widely used for training in multivariate data analysis, with many online tutorials.\nThe social reasons to learn programming also apply to programming for research.",
    "crumbs": [
      "05. Additional resources"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "01. Introduction",
    "section": "",
    "text": "As research techniques and data collection have become almost completely digital and analysis methods grow more sophisticated, it is critical that scientists develop three skills: data visualization, statistics, and coding. Unfortunately, many undergraduate biology programs emphasize the memorization of numerous facts, while failing to offer courses in data graphics, estimation statistics, or scientific programming. In this session, we offer a basic orientation on these topics.\n# Python is a general programming language. \n# This website was generated with Python tools.\n# This window is a tiny demonstration of how Python is integrated with the web.\nfrom IPython.display import HTML\n\nHTML(\"\"\"\n&lt;iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/kgicuytCkoY\" \n        frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;\n\"\"\")",
    "crumbs": [
      "01. Introduction"
    ]
  },
  {
    "objectID": "introduction.html#before-class",
    "href": "introduction.html#before-class",
    "title": "01. Introduction",
    "section": "Before class",
    "text": "Before class\nIf any issues can’t be resolved with the below steps, we can work on it in the class together.\n\nGetting the necessary software\n\nYou’ll need to get set up with a version-control system. Go to GitHub and get an account. Download and install GitHub Desktop.\nRetrieve the course materials from GitHub. Go to the course repository (“repo”) at https://github.com/ACCLAB/moda. Click the green Code button and then select Open with GitHub Desktop. You will be prompted to select a directory for the local repository. If you are using a PC it can be something like this:  If you are using a mac, it can be something like //Users/YOURUSERNAME/Documents/GitHub/moda.\nTo get set up with Python and Jupyter notebooks, install the Anaconda Distribution on your laptop.\nOpen Anaconda Navigator and open a terminal window by clicking on Environments &gt; base (root), and then clicking on the green triangle and select Open Terminal.\n\n \n\nGo to your moda directory (replace the path with your own actual path) and install it with pip: cd Documents/GitHub/moda pip install .\n\n\n\nChecking out the notebooks\n\nlaunch JupyterLab by clicking on it.\n\n JupyterLab will open in a browser tab.\n\nIn the File Browser panel in JupyterLab, navigate to the folder where you cloned the course repo (refer to step 2). Double click on ‘nbs’. You should see a list of notebook files. Open “02_Quick_tour_of_the_Notebook.ipynb” by double-clicking on the icon shown in the JupyterLab browser window. \nWork through the notebook. Familiarize yourself with basic Python, and with working in the JupyterLab environment.\n\n\n\nReading about the python packages we will use\n\nRead about pandas, matplotlib, and seaborn.\nRead our papers on estimation statistics here and here.",
    "crumbs": [
      "01. Introduction"
    ]
  },
  {
    "objectID": "introduction.html#in-class",
    "href": "introduction.html#in-class",
    "title": "01. Introduction",
    "section": "In class",
    "text": "In class\n\nSession Coding. Bring your laptop to class, and we will go through the examples in Data Analysis with Jupyter.\nLecture. Crash course on the history of and key issues in data visualization.\nShort introduction to estimation and LLMs. Introduction to estimation statistics web app and python package.\n\nOptional: Try JupyterLite, an experimental web version of JupyterLab, with a class notebook here.",
    "crumbs": [
      "01. Introduction"
    ]
  },
  {
    "objectID": "bonus_pca_analysis.html",
    "href": "bonus_pca_analysis.html",
    "title": "03b. Bonus PCA Analysis",
    "section": "",
    "text": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()",
    "crumbs": [
      "03b. Bonus PCA Analysis"
    ]
  },
  {
    "objectID": "bonus_pca_analysis.html#load-libraries",
    "href": "bonus_pca_analysis.html#load-libraries",
    "title": "03b. Bonus PCA Analysis",
    "section": "",
    "text": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()",
    "crumbs": [
      "03b. Bonus PCA Analysis"
    ]
  },
  {
    "objectID": "bonus_pca_analysis.html#question-4-how-do-we-concisely-describe-the-penguin-dataset",
    "href": "bonus_pca_analysis.html#question-4-how-do-we-concisely-describe-the-penguin-dataset",
    "title": "03b. Bonus PCA Analysis",
    "section": "Question 4: how do we concisely describe the penguin dataset?",
    "text": "Question 4: how do we concisely describe the penguin dataset?\n(Or, dimension reduction with Principle Component Analysis)\nFor this we go back to the origianl metrics including all lengths and body mass. We have 4 dimensions we measured the penguins on, and their relationship with each other is a little complex. Many of the metrics are correlated with each other, indicating redudancy between them. What if we want a more concise way of describing the data? We can try to reduce the dimensions to fewer that are orthogonal to each other, along the first of which the data has the most variance.\nRead more about PCA here\n\nfrom sklearn.decomposition  import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\npenguins_dropna = penguins.dropna()\nX = penguins_dropna[[\"bill_length_mm\",  \"bill_depth_mm\",    \"flipper_length_mm\", \"body_mass_g\"]] \nY = penguins_dropna[\"species\"] \nscaler = StandardScaler() # Create a StandardScaler object.\nscaler.fit(X) # Fit the StandardScaler object to the data.\nX_scaled = scaler.transform(X) # Generating a matrix with standardized values (z-scores). Z-scores allow us to compare all the metrics on the same scale.\n\nFirst we generate all the principle components we can, in this case it is 4, and we check the explanatory power of including more and more of them.\n\npca4 = PCA(n_components=4)\npca4.fit(X_scaled)\nf = plt.plot(pca4.explained_variance_ratio_.cumsum(), \"o-\")\nplt.xlabel(\"Number of Principal Components\");\nplt.ylabel(\"Cumulative Explained Variance Ratio\");\nplt.title(\"Scree Plot of Penguin Measurements\");\n\n\n\n\n\n\n\n\nWe see that 2 components explain vast majority of the variance. So we are going to plot our original data into the PC1 v PC2 space. Now let’s project the data into this new, more concise space.\n\npca2 = PCA(n_components=2)\npca2.fit(X_scaled)\nPCs = pca2.fit_transform(X_scaled)\nX_scaled_df = pd.DataFrame(X_scaled, columns = [\"bill_length_mm\",   \"'bill_depth_mm\",   \"flipper_length_mm\", \"body_mass_g\"])\nPCs_df = pd.DataFrame(PCs, columns = [\"PC1\", \"PC2\"])\npenguins_pca = pd.concat([X_scaled_df, Y, PCs_df], axis = 1)\n\nf, ax = plt.subplots(1, figsize = (5, 5))\nsns.scatterplot(x=PCs[:, 0], y=PCs[:, 1], hue=Y, alpha = 0.7)\nax.set_xlabel(\"PC1\");\nax.set_ylabel(\"PC2\");\n\n\n\n\n\n\n\n\nMathematically, PC1 and PC2 are linear combinations of the original metrics. We also want to know how much of each of the original metrics contribute to each of the PCs. We calculate a loading matrix for this puropse.\n\nloading_matrix = pd.DataFrame(pca2.components_.T, columns = [\"PC1\", \"PC2\"], index = [\"bill_length_mm\",  \"'bill_depth_mm\",   \"flipper_length_mm\", \"body_mass_g\"])\n\nf = sns.heatmap(loading_matrix, cmap=\"coolwarm\", annot=True)\nplt.title(\"Heatmap of Loading Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that PC1 is mostly coming from filpper length and body mass, and PC2 is mostly coming from bill depth.",
    "crumbs": [
      "03b. Bonus PCA Analysis"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html",
    "href": "quick_tour_of_the_notebook.html",
    "title": "02. A Quick Tour of The Notebook",
    "section": "",
    "text": "If you’re reading this properly, then you’ve installed Anaconda correctly. You are looking at a Jupyter notebook, which is a user-friendly combination of code and text that helps you perform data analysis and visualization easily!",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#welcome-to-the-jupyter-notebook",
    "href": "quick_tour_of_the_notebook.html#welcome-to-the-jupyter-notebook",
    "title": "02. A Quick Tour of The Notebook",
    "section": "",
    "text": "If you’re reading this properly, then you’ve installed Anaconda correctly. You are looking at a Jupyter notebook, which is a user-friendly combination of code and text that helps you perform data analysis and visualization easily!",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#introducing-the-notebook",
    "href": "quick_tour_of_the_notebook.html#introducing-the-notebook",
    "title": "02. A Quick Tour of The Notebook",
    "section": "Introducing The Notebook",
    "text": "Introducing The Notebook\nA Jupyter notebook consists of text and images. The text can be Markdown or code.\nYou can include images as a weblink:\n\n\n\nclaridge-chang-lab-logo\n\n\nIf you’re not connected to the internet, the image above won’t render.\n\nDouble-click on this chunk of text. You should enter “edit mode” which allows you to change the text.\nHit Shift+Enter to render the change.",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#running-cells",
    "href": "quick_tour_of_the_notebook.html#running-cells",
    "title": "02. A Quick Tour of The Notebook",
    "section": "Running Cells",
    "text": "Running Cells\nFirst, we need to explain how to run cells. Try to run the cell below!\nNotice how the cell bracket on the left first becomes an asterisk “*” to indicate the cell is running.\nOnce the cell has completed running, the asterisk is replaced by a number (in this case, “1”). This is a running count of the number of cells you have run in this notebook.\n\nimport pandas as pd # Don't worry about this line yet. We'll 3explain it later below!\n\n\nprint(\"Hi! This is a cell. Press the ▶ button in the toolbar above to run it.\")\n\nHi! This is a cell. Press the ▶ button in the toolbar above to run it.\n\n\nYou can also run a cell with Shift+Enter.\nOne of the most useful things about IPython notebook is its tab completion.\nTry this: remove the “#” in the cell below (called uncommenting) and click just after read_csv( in the cell below and press Shift+Tab.\n\n# pd.read_csv(\n\nYou should see this: \n\nThis is the documentation for the function pd.read_csv(). You should be able to scroll within the box.\nYou can also perform tab completion for function names.\nAfter removing the #, just after pd.r in the cell below and press Tab.\n\n# pd.r\n\nYou should see this:\n\n\n\nUsing Tab completion",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#saving-your-work",
    "href": "quick_tour_of_the_notebook.html#saving-your-work",
    "title": "02. A Quick Tour of The Notebook",
    "section": "Saving Your Work",
    "text": "Saving Your Work\nAs of the latest stable version, JupyterLab will autosave your notebook! But you should always click the 💾 button or hit Ctrl+S regularly.",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#introducing-python",
    "href": "quick_tour_of_the_notebook.html#introducing-python",
    "title": "02. A Quick Tour of The Notebook",
    "section": "Introducing Python",
    "text": "Introducing Python\n\n# This is a code cell. You can change the cell type in the dropdown menu above.\n# In Python, anything following a `#` is a comment; it is ignored. \n# Below, we demonstrate a simple addition of two variables.\n\na = 5\nb = 6\nprint(a + b)\nprint(a * b)\n\n11\n30\n\n\nTry changing the values of a and b, and hit Shift+Enter or the Play button ▶ above.\nCheck to see if the values of a + b and a * b are as expected.\nCongratulations! 🎉 🎊\nYou are now officially a programmer!!!",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#for-loops",
    "href": "quick_tour_of_the_notebook.html#for-loops",
    "title": "02. A Quick Tour of The Notebook",
    "section": "FOR loops",
    "text": "FOR loops\nOne of the key advantages of coding is the ability to automate repetitive processes with loops.\n\nfor i in range(10):\n    print(i, i*2, i*3)\n\n0 0 0\n1 2 3\n2 4 6\n3 6 9\n4 8 12\n5 10 15\n6 12 18\n7 14 21\n8 16 24\n9 18 27",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#quick-introduction-to-arrays",
    "href": "quick_tour_of_the_notebook.html#quick-introduction-to-arrays",
    "title": "02. A Quick Tour of The Notebook",
    "section": "Quick Introduction to Arrays",
    "text": "Quick Introduction to Arrays\nPython has several different types of data structures. An important data structure is the list.\n\nmy_list = [1, 2, 3, 4, \"John\", \"Mary\", 1984]\n\nYou can access items of this list with using a 0-indexed notation. That is, the first item has an index of 0.\n\nmy_list[0]\n\n1\n\n\n\nmy_list[1]\n\n2\n\n\nPython allows negative-numerical indexing, which accesses the list in reverse.\n\nmy_list[-1]\n\n1984\n\n\n\nmy_list[-2]\n\n'Mary'\n\n\nYou can append items to the list.\n\nmy_list.append(\"new item\")\n\nmy_list\n\n[1, 2, 3, 4, 'John', 'Mary', 1984, 'new item']\n\n\nYou can also remove items from the list.\n\nmy_list.remove(4)\n\nmy_list.remove(\"John\")\n\nmy_list\n\n[1, 2, 3, 'Mary', 1984, 'new item']\n\n\nYou can also combine lists.\n\n[1, 2, 3] + [\"Four\", \"cinco\", \"六\"]\n\n[1, 2, 3, 'Four', 'cinco', '六']",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#quick-introduction-to-dictionaries",
    "href": "quick_tour_of_the_notebook.html#quick-introduction-to-dictionaries",
    "title": "02. A Quick Tour of The Notebook",
    "section": "Quick Introduction to Dictionaries",
    "text": "Quick Introduction to Dictionaries\nAnother important data structure is the dictionary. It is often referred to as a dict. As the name suggests, you can look up values with keywords.\n\nmy_dict = {\"John\": 99,\n           \"Mary\": 100,\n           \"Address\": \"8 College Road, S(169857)\",\n           \"Fragments\": [\"attagagacca\", \"ggctttcta\", \"ttctcaatggt\"]}\n\n\nmy_dict[\"John\"]\n\n99\n\n\n\nmy_dict[\"Address\"]\n\n'8 College Road, S(169857)'\n\n\n\nmy_dict[\"Fragments\"]\n\n['attagagacca', 'ggctttcta', 'ttctcaatggt']\n\n\nYou can add values to a list by assigning it to a keyword.\n\nmy_dict[\"Susan\"] = 1000\n\nmy_dict\n\n{'John': 99,\n 'Mary': 100,\n 'Address': '8 College Road, S(169857)',\n 'Fragments': ['attagagacca', 'ggctttcta', 'ttctcaatggt'],\n 'Susan': 1000}\n\n\nRemoving dictionary entries with the del command.\n\ndel my_dict[\"John\"]\n\nmy_dict\n\n{'Mary': 100,\n 'Address': '8 College Road, S(169857)',\n 'Fragments': ['attagagacca', 'ggctttcta', 'ttctcaatggt'],\n 'Susan': 1000}\n\n\n\nQuiz\nHow do I remove ‘ggctttcta’ from my_dict[\"Fragments\"]?",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#importing-libraries",
    "href": "quick_tour_of_the_notebook.html#importing-libraries",
    "title": "02. A Quick Tour of The Notebook",
    "section": "Importing Libraries",
    "text": "Importing Libraries\nPython has a large selection of libraries for scientific computing and data visualization. These libraries have to be manually imported into your session.\n\nimport pandas as pd\n# This translates into English as 'Hey Python, load the library `pandas`, and call it `pd` for short!'\n\nimport matplotlib.pyplot as plt\n# In English: 'Python, there's a submodule `pyplot` in the library `matplotlib`. Import this submodule as `plt`.\n\nimport seaborn as sns\n# Now, can you translate this line of Python into English?\n\n## For some reason, this gave an error in nbdev_test\n# %matplotlib inline\n## This is known as a \"IPython magic command\". Some magic commands control how the notebook behaves. \n## This particular line tells the notebook to render any output by `matplotlib` as an inline image.",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "quick_tour_of_the_notebook.html#further-reading",
    "href": "quick_tour_of_the_notebook.html#further-reading",
    "title": "02. A Quick Tour of The Notebook",
    "section": "Further Reading",
    "text": "Further Reading\nRead more about the JupyterLab interface.\nJupyter follows in the tradition of literate programming, the idea that code should be a piece of literature that incorporates prose, images, and code all in a single document.\nMore about magic commands in Jupyter.\nWhy is this named “Jupyter”? From this link:\n\nThe name has its origins in a few different places. First, the names comes from the planet Jupiter. We wanted to pick a name that evoked the traditions and ideas of science. Second, the core programming languages supported by Jupyter are Julia, Python and R. While the name Jupyter is not a direct acronym for these languages, it nods its head in those directions. In particular, the “y” in the middle of Jupyter was chosen to honor our Python heritage. Third, Galileo was the first person to discover the moons of Jupiter. His publication on the moons of Jupiter is an early example of research that includes the underlying data in the publication. This is one of the core ideas and requirements for scientific reproducibility. Reproducibility is one of the main focuses of our project.",
    "crumbs": [
      "02. A Quick Tour of The Notebook"
    ]
  },
  {
    "objectID": "data_analysis_with_jupyter_and_python.html",
    "href": "data_analysis_with_jupyter_and_python.html",
    "title": "03. Data Analysis with Jupyter and Python",
    "section": "",
    "text": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins",
    "crumbs": [
      "03. Data Analysis with Jupyter and Python"
    ]
  },
  {
    "objectID": "data_analysis_with_jupyter_and_python.html#load-libraries",
    "href": "data_analysis_with_jupyter_and_python.html#load-libraries",
    "title": "03. Data Analysis with Jupyter and Python",
    "section": "",
    "text": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins",
    "crumbs": [
      "03. Data Analysis with Jupyter and Python"
    ]
  },
  {
    "objectID": "data_analysis_with_jupyter_and_python.html#loading-data",
    "href": "data_analysis_with_jupyter_and_python.html#loading-data",
    "title": "03. Data Analysis with Jupyter and Python",
    "section": "Loading data",
    "text": "Loading data\nLet’s load an example dataset: the palmerpenguins dataset. It was created by Allison Horst, Alison Hill and Kristen Gorman from data that Gorman collected on the islands of the Palmer Archipelago in Antarctica between 2007 and 2009. The multivariate dataset includes characteristics of the penguins including species and sex, body size measurements such as bill length, bill depth, flipper length and body mass, as well as location and year of measurements. You can read more about this dataset here.\n\n\n\nFigure 1. Artwork by @allison_horst\n\n\n\n\n\nFigure 2. Artwork by @allison_horst\n\n\n\npenguins = load_penguins() \n\n# if you had trouble installing the package, you can also read in the data from the csv file by uncommenting the line below.\n# penguins = pd.read_csv(\"penguins.csv\")\n\nYou have created a new object known as a pandas DataFrame, with the contents of the package or the CSV. Think of it as a spreadsheet, but with a lot more useful features for data analysis. It has several methods we can use to handle and analyse the data.\nRead more about the pandas DataFrame object here.",
    "crumbs": [
      "03. Data Analysis with Jupyter and Python"
    ]
  },
  {
    "objectID": "data_analysis_with_jupyter_and_python.html#overview-of-data",
    "href": "data_analysis_with_jupyter_and_python.html#overview-of-data",
    "title": "03. Data Analysis with Jupyter and Python",
    "section": "Overview of data",
    "text": "Overview of data\nWe can peek at the data using the .head() function.\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\nWe notice that there are a lot of different measurements in the form of columns. For the first section we are going to ask questions about how bill and flipper measurements vary with species, so we”ll make a smaller dataframe containing only the species column and columns containing length measurements.\n\npenguins_lengths = penguins[[\"species\", \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]]\npenguins_lengths.head() # Gives us the first 5 rows of the dataframe.\n# penguins.head(10) # Gives us the first 5 rows of the dataframe.\n\n\n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n0\nAdelie\n39.1\n18.7\n181.0\n\n\n1\nAdelie\n39.5\n17.4\n186.0\n\n\n2\nAdelie\n40.3\n18.0\n195.0\n\n\n3\nAdelie\nNaN\nNaN\nNaN\n\n\n4\nAdelie\n36.7\n19.3\n193.0\n\n\n\n\n\n\n\n\nData cleanup\nWe noticed that the DataFrame above contain NaN values. This means these entries are missing data. Let’s first find out which rows contain NaN values:\n\npenguins_lengths[penguins_lengths.isnull().any(axis = 1)] # Gives us the rows with at least one missing values.\n\n\n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n3\nAdelie\nNaN\nNaN\nNaN\n\n\n271\nGentoo\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nMissing data sometimes create issues for analyses and visualiszation, so we want to drop all the rows that contain NaN values with the .dropna() function:\n\npenguins_lengths = penguins_lengths.dropna().reset_index(drop = True) # Drop rows with missing values and re-numbers the rows.\npenguins_lengths.head()\n\n\n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n0\nAdelie\n39.1\n18.7\n181.0\n\n\n1\nAdelie\n39.5\n17.4\n186.0\n\n\n2\nAdelie\n40.3\n18.0\n195.0\n\n\n3\nAdelie\n36.7\n19.3\n193.0\n\n\n4\nAdelie\n39.3\n20.6\n190.0\n\n\n\n\n\n\n\n\n\nQuick summary of the data\nFirst we get a summary of the data with the .describe() function.\n\npenguins_lengths.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n\n\nstd\n5.459584\n1.974793\n14.061714\n\n\nmin\n32.100000\n13.100000\n172.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n\n\n\n\n\n\n\nSince we are interested in the species, we take a look at what is in the species column. There are three species.\n\npenguins_lengths.species.unique() # Gives us the unique values in the species column.\n\narray(['Adelie', 'Gentoo', 'Chinstrap'], dtype=object)",
    "crumbs": [
      "03. Data Analysis with Jupyter and Python"
    ]
  },
  {
    "objectID": "data_analysis_with_jupyter_and_python.html#question-1-how-long-are-the-bills-of-penguins-of-different-species",
    "href": "data_analysis_with_jupyter_and_python.html#question-1-how-long-are-the-bills-of-penguins-of-different-species",
    "title": "03. Data Analysis with Jupyter and Python",
    "section": "Question 1: how long are the bills of penguins of different species?",
    "text": "Question 1: how long are the bills of penguins of different species?\n(Or, looking at one dependent variables against one independent variable)\nWe will approach Question 1 with a very simple analysis workflow: split-apply-combine. Many of our scientific experiments also follow this workflow: We collect some measuments (value) from individuals belonging to different groups (key), split the data according to the grouping, apply a summary function to each group, and then aggregate the results. After that you can visualise the results. For example, Figure 3 shows how you would plot a simple bar plot with the bill length data.\n\n\n\nFigure 3. The split-apply-combine work flow.\n\n\n\npenguins_lengths.groupby(\"species\").mean() # Gives us the mean values for each species.\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\nspecies\n\n\n\n\n\n\n\nAdelie\n38.791391\n18.346358\n189.953642\n\n\nChinstrap\n48.833824\n18.420588\n195.823529\n\n\nGentoo\n47.504878\n14.982114\n217.186992\n\n\n\n\n\n\n\n\npenguins_lengths.groupby(\"species\").sem() # Gives us the standard error of the mean for each species.\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\nspecies\n\n\n\n\n\n\n\nAdelie\n0.216745\n0.099010\n0.532173\n\n\nChinstrap\n0.404944\n0.137687\n0.864869\n\n\nGentoo\n0.277882\n0.088474\n0.584731\n\n\n\n\n\n\n\n\nPlot an old-fashioned bar plot, which shows the mean and S.E.M. of each group\nLuckily, a lot of these plotting functions already exist in data visualisation packages such as seaborn:\n\n# ax1 = sns.barplot(data = penguins_lengths, \n#                   x = \"species\", \n#                   y = \"bill_length_mm\",\n#                   errorbar = \"se\");\n\n# # Axes should always be labelled.\n# ax1.set(xlabel=\"Species\", ylabel=\"Mean Bill length (mm)\", title = \"Bar Plot of Penguin Bill Length by Species\");\n\nax1 = sns.barplot(data=penguins_lengths, \n                  x=\"species\", \n                  y=\"bill_length_mm\",\n                  errorbar=\"sd\");  # Use \"sd\" for standard deviation or a numeric value for confidence interval\n\n# Axes should always be labelled.\nax1.set(xlabel=\"Species\", ylabel=\"Mean Bill length (mm)\", title=\"Bar Plot of Penguin Bill Length by Species\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPlot a box plot, which shows the data quartiles\n\nax2 = sns.boxplot(data = penguins_lengths, \n                    x = \"species\", \n                    y = \"bill_length_mm\",\n                   hue = \"species\");\n\nax2.set(xlabel=\"Species\", ylabel=\"Mean Bill length (mm)\", title = \"Box Plot of Penguin Bill Length by Species\");\n\n\n\n\n\n\n\n\n\n\nPlot a swarmplot, which shows all the data points\n\nax3 = sns.swarmplot(data = penguins_lengths, \n                    x = \"species\", \n                    y = \"bill_length_mm\",\n                   hue = \"species\");\n\nax3.set(xlabel=\"Species\", ylabel=\"Mean Bill length (mm)\", title = \"Swarm Plot of Penguin Bill Length by Species\");",
    "crumbs": [
      "03. Data Analysis with Jupyter and Python"
    ]
  },
  {
    "objectID": "data_analysis_with_jupyter_and_python.html#question-2-how-do-all-the-length-metrics-vary-with-species",
    "href": "data_analysis_with_jupyter_and_python.html#question-2-how-do-all-the-length-metrics-vary-with-species",
    "title": "03. Data Analysis with Jupyter and Python",
    "section": "Question 2: How do all the length metrics vary with species?",
    "text": "Question 2: How do all the length metrics vary with species?\n(Or, looking at more metrics systematically.)\n\npenguins_lengths.head() # Let's revisit the DataFrame\n\n\n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n0\nAdelie\n39.1\n18.7\n181.0\n\n\n1\nAdelie\n39.5\n17.4\n186.0\n\n\n2\nAdelie\n40.3\n18.0\n195.0\n\n\n3\nAdelie\n36.7\n19.3\n193.0\n\n\n4\nAdelie\n39.3\n20.6\n190.0\n\n\n\n\n\n\n\nWe don’t have to visualize only 1 metric at a time. We can look at all of the metrics at the same time. The plotting package seaborn has a function catplot() that does this automatically for you.\n\n# `catplot` is short for \"categorical plot\", \n# where either the x-axis or y-axis consists of categories.\n\n# This will produce an error - intentionally showing why we need to reshape the data\ntry:\n    ax4 = sns.catplot(data=penguins_lengths, \n                kind=\"bar\",   # there are several types of plots.\n                # errorbar=\"sd\",      # plot the error bars as ± standard deviation, use this line if you have seaborn 0.12.x\n                ci=\"sd\",      # plot the error bars as ± standard deviations, use this line if you have seaborn 0.11.x.\n                col=\"species\" # plot each species as its own column.\n               )\n    ax4.set_axis_labels(\"\", \"Length (cm)\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    print(\"\\n(This error is generated because seaborn disallows catplot with 'wide' data)\")\n\nError: The following variable cannot be assigned with wide-form data: `col`\n\n(This error is generated because seaborn disallows catplot with 'wide' data)\n\n\n\nLimits of wide-format data\nThe default ‘wide’ penguins DataFrame cannot be plotted by seaborn.catplot. To directly compare metrics between species, we need to reshape the data. Read more about wide and long data here.\n\n\nReshaping Data\nOur penguins_lengths dataframe is in the wide-form (below left) and we want to turn it into the long-form (below right). In the original penguins_lengths dataframe, the data is organised by individuals (one individual penguin in each row) and the columns contain a mixture of variables that describe that individual. In a long-form dataframe, each row is an observation or data point, and each column is a variable that describe the data point. (Please read Hadley Wickham’s article to learn more about tidiness of datasets.)\n\n\n\nFigure 4. The Long-form vs the Wide-form of your data\n\n\n\npenguins_lengths # This is our wide table.\n\n\n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n0\nAdelie\n39.1\n18.7\n181.0\n\n\n1\nAdelie\n39.5\n17.4\n186.0\n\n\n2\nAdelie\n40.3\n18.0\n195.0\n\n\n3\nAdelie\n36.7\n19.3\n193.0\n\n\n4\nAdelie\n39.3\n20.6\n190.0\n\n\n...\n...\n...\n...\n...\n\n\n337\nChinstrap\n55.8\n19.8\n207.0\n\n\n338\nChinstrap\n43.5\n18.1\n202.0\n\n\n339\nChinstrap\n49.6\n18.2\n193.0\n\n\n340\nChinstrap\n50.8\n19.0\n210.0\n\n\n341\nChinstrap\n50.2\n18.7\n198.0\n\n\n\n\n342 rows × 4 columns\n\n\n\n\n# The code in this section turns a wide table into a long table.\npenguins_tidy = pd.melt(penguins_lengths.reset_index(), \n                    id_vars=[\"index\",\"species\"], \n                    var_name=\"metric\", \n                    value_name=\"cm\")\npenguins_tidy = penguins_tidy.rename(columns = {\"index\": \"ID\"})\n\n\npenguins_tidy # This is our long table.\n\n\n\n\n\n\n\n\nID\nspecies\nmetric\ncm\n\n\n\n\n0\n0\nAdelie\nbill_length_mm\n39.1\n\n\n1\n1\nAdelie\nbill_length_mm\n39.5\n\n\n2\n2\nAdelie\nbill_length_mm\n40.3\n\n\n3\n3\nAdelie\nbill_length_mm\n36.7\n\n\n4\n4\nAdelie\nbill_length_mm\n39.3\n\n\n...\n...\n...\n...\n...\n\n\n1021\n337\nChinstrap\nflipper_length_mm\n207.0\n\n\n1022\n338\nChinstrap\nflipper_length_mm\n202.0\n\n\n1023\n339\nChinstrap\nflipper_length_mm\n193.0\n\n\n1024\n340\nChinstrap\nflipper_length_mm\n210.0\n\n\n1025\n341\nChinstrap\nflipper_length_mm\n198.0\n\n\n\n\n1026 rows × 4 columns\n\n\n\n\nax5 = sns.catplot(data=penguins_tidy, \n            x=\"metric\", \n            y=\"cm\", \n            hue=\"species\",\n            kind=\"bar\",\n            errorbar=\"sd\",\n            aspect=1.5\n           )\n\n# Get the figure's axes\nax = ax5.axes.flat[0]\n\n# Set both the tick positions and labels\nax.set_xticks(range(len([\"Bill Length\", \"Bill Depth\", \"Flipper Length\"])))\nax.set_xticklabels([\"Bill Length\", \"Bill Depth\", \"Flipper Length\"])\n\n# Set the other labels\nax5.set(xlabel=\"Measurements\", \n        ylabel=\"Length in mm\", \n        title=\"Cat plot of Penguin Body Measurements by Species\")\n\n\n\n\n\n\n\n\ncatplot allows you to do plot different kinds of plots in the same format easily. Let’s say we want to see a box plot:\n\nax6 = sns.catplot(data=penguins_tidy, \n            kind=\"box\", \n            x=\"metric\",\n            y=\"cm\",\n            hue=\"species\",\n            aspect=1.5\n           )\n\n# Get the figure's axes\nax = ax6.axes.flat[0]\n\n# Set both the tick positions and labels\nax.set_xticks(range(len([\"Bill Length\", \"Bill Depth\", \"Flipper Length\"])))\nax.set_xticklabels([\"Bill Length\", \"Bill Depth\", \"Flipper Length\"])\n\n# Set the other labels\nax6.set(xlabel=\"Measurements\", \n        ylabel=\"Length in cm\", \n        title=\"Catplot of Penguin Body Measurements by Species\")",
    "crumbs": [
      "03. Data Analysis with Jupyter and Python"
    ]
  },
  {
    "objectID": "data_analysis_with_jupyter_and_python.html#question-3-how-does-bill-length-vary-with-flipper-length",
    "href": "data_analysis_with_jupyter_and_python.html#question-3-how-does-bill-length-vary-with-flipper-length",
    "title": "03. Data Analysis with Jupyter and Python",
    "section": "Question 3: how does bill length vary with flipper length?",
    "text": "Question 3: how does bill length vary with flipper length?\n(Or, how do we explore the correlation between metrics?)\nNext to the categorical plot, the scatter plot is a very useful visualization tool for biological experiments. Often we want to know how one variable is correlated with another, we can then use a scatterplot to easily take a quick look.\n\n# Draw a scatteplot of petal width versus length with a simple linear regression line\nax7 = sns.regplot(data=penguins, \n                  x=\"bill_length_mm\", \n                  y=\"flipper_length_mm\",\n                  ci=95)\n\nfrom scipy.stats import pearsonr\npearsonsr, p = pearsonr(penguins.dropna()[\"bill_length_mm\"], penguins.dropna()[\"flipper_length_mm\"])\nax7.set(xlabel=\"Bill Length (mm)\", ylabel=\"Flipper Length (mm)\", title = \"Correlation between Bill Length and Flipper Length\");\n# ax7. text(35, 220, f\"r = {pearsonsr:.2f},\\np = {p:.2f}\") # Uncomment this line if you want to also present p-value.\nax7.text(35, 220, f\"r = {pearsonsr:.2f}\");\n\n\n\n\n\n\n\n\nWe see some clustering of the data points and we suspect the clusters correspond to the different species of penguins. Let”s colour the points by species.\n\nfor s in penguins.species.unique():\n    ax8 = sns.regplot(data=penguins.loc[penguins.species == s], \n                  ci=95,\n                  x=\"bill_length_mm\", \n                  y=\"flipper_length_mm\", label = s)\n    ax8.legend()\n    \nax8.set(xlabel=\"Bill Length (mm)\", ylabel=\"Flipper Length (mm)\", title = \"Correlation between Bill Length and Flipper Length by Species\");\n\n\n\n\n\n\n\n\n\nSeaborn allows you to do that more systematically with pairplot\nThis is like doing a scatter plot for each pair of the variables in one go. On the diagonal, distributions of values within each species group are plotted for each variable.\n\nfig = sns.pairplot(penguins, hue=\"species\")\n\n\n\n\n\n\n\n\nWe can see from the pair plots above that the relationshipship between all the metrics are complex and difficult to summary. If you have time, you can explore the bonus notebook, where we discuss how to concisely describe the data by reducing the dimentionality of this dataset Bonus PCA Analysis",
    "crumbs": [
      "03. Data Analysis with Jupyter and Python"
    ]
  },
  {
    "objectID": "data_analysis_with_jupyter_and_python.html#towards-publication-ready-plots",
    "href": "data_analysis_with_jupyter_and_python.html#towards-publication-ready-plots",
    "title": "03. Data Analysis with Jupyter and Python",
    "section": "Towards Publication-Ready Plots",
    "text": "Towards Publication-Ready Plots\nTry to achieve as much of the final figure requirements as possible via code\n\npenguins_all_metrics = penguins[[\"species\", \"bill_depth_mm\", \"bill_length_mm\", \"flipper_length_mm\", \"body_mass_g\"]].dropna().reset_index(drop = True)\nall_metrics = [\"bill_depth_mm\", \"bill_length_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n\nall_metrics\n\n['bill_depth_mm', 'bill_length_mm', 'flipper_length_mm', 'body_mass_g']\n\n\n\ny_titles = [ \"Bill Length (mm)\", \"Bill Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\nletters = [\"A\", \"B\", \"C\", \"D\"]\n\n\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(2, 2, figsize=(10, 10))\n\nall_axes = ax.flatten()\n\nfor i, metric in enumerate(all_metrics):\n    \n    current_axes = all_axes[i]\n    \n    sns.swarmplot(data=penguins_all_metrics, size = 3.5, \n                  x=\"species\", y=metric, hue = \"species\",\n                  ax=current_axes)\n    ylim = current_axes.get_ylim()\n    current_axes.set(ylabel=y_titles[i])\n\n    if i != 0:\n        legend = current_axes.get_legend()\n        if legend is not None:  # Only try to remove if legend exists\n            legend.remove()\n    current_axes.text(-1, ylim[1], letters[i], fontsize = 15, fontweight = \"semibold\")\n\n\n\n\n\n\n\n\n\nf.savefig(\"myplot.svg\")\nf.savefig(\"myplot.png\", dpi = 300)\n\n\nYou have begun your journey as a data scientist\nYou can continue your data-visualization development with Claus Wilke’s free online book; it is not only a great introduction to data visualization, but also a style guide. It is written for the R programming language, but the principles are universal, and Python libraries like seaborn offer many of the same features.\n\n\n\nFlorence Nightingale’s Mortality Diagram\n\n\nThe “coxcomb” diagram shows the causes of mortality for British soldiers during the Crimean War 1853-56, with each wedge representing a month. The red sections indicate deaths from wounds, black from other causes, and the blue sections indicate deaths from preventable diseases. Nightingale used this visualization to illustrate how poor conditions were killing more soldiers than battle itself, thereby convincing military officials to improve sanitation in army hospitals. This pioneering work help establish data visualization as a powerful tool for communication and evidence-based decision making. It was created by Florence Nightingale in 1858, and represents one of the earliest examples of data visualization used for medical reform.",
    "crumbs": [
      "03. Data Analysis with Jupyter and Python"
    ]
  }
]